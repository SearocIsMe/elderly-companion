services:
  llamacpp:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda   # ← CUDA 版镜像
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    command: >
      -m /models/qwen2.5-3b-instruct-gguf/Qwen2.5-3B-Instruct-Q4_K_M.gguf
      -c ${CTX_LEN:-1024} -ngl ${LLAMA_NGL:-28} -t ${LLAMA_THREADS:-12}
      --host 0.0.0.0 --port 8001
